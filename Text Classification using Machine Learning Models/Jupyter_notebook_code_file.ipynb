{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59764035-b283-49c9-944c-a56d3b5b4544",
   "metadata": {},
   "source": [
    "# PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be836ef-f26a-45cb-b066-f76ae4b299ff",
   "metadata": {},
   "source": [
    "# Topic : IMBD dataset analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cb397a-b4be-483b-a9d0-ace9627df5ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71492e29-376c-4b0a-88fa-be1b03177bd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a17ec3-ddad-4349-a349-3f3150878913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a468e9a3-ff6b-475f-9b75-d7cb403c04d6",
   "metadata": {},
   "source": [
    "### Reading the Csv fiel and loading it to pandas dataframe :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47ceb0a9-377d-4382-a3a9-536d0ccabc49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb = pd.read_csv(\"IMDB_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d85776-0793-4a20-be0c-98eca2264838",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3278</th>\n",
       "      <td>Though a fan of shock and gore, I found this m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8462</th>\n",
       "      <td>Terrible movie. Nuff Said.&lt;br /&gt;&lt;br /&gt;These Li...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11676</th>\n",
       "      <td>I don't understand why people would praise thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>First of all, I'd like to say that I really en...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17770</th>\n",
       "      <td>In complete contrast to the previous correspon...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>I'm afraid I only stayed to watch the first ho...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19081</th>\n",
       "      <td>Artistically speaking, this is a beautiful mov...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10542</th>\n",
       "      <td>I know a lot of people like this show and i ap...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>Spoilers of both this and The Matrix follow.&lt;b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7574</th>\n",
       "      <td>***SPOILERS*** ***SPOILERS*** Continued...&lt;br ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "3278   Though a fan of shock and gore, I found this m...  negative\n",
       "8462   Terrible movie. Nuff Said.<br /><br />These Li...  negative\n",
       "11676  I don't understand why people would praise thi...  negative\n",
       "7047   First of all, I'd like to say that I really en...  negative\n",
       "17770  In complete contrast to the previous correspon...  positive\n",
       "4471   I'm afraid I only stayed to watch the first ho...  negative\n",
       "19081  Artistically speaking, this is a beautiful mov...  positive\n",
       "10542  I know a lot of people like this show and i ap...  negative\n",
       "1832   Spoilers of both this and The Matrix follow.<b...  negative\n",
       "7574   ***SPOILERS*** ***SPOILERS*** Continued...<br ...  negative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c53476-966b-4438-95b9-9882b21389bf",
   "metadata": {},
   "source": [
    "### Performing the EDA of the Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbc4cac-4790-4665-8e85-8d0e2207a73c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f42826-6ed7-46c6-8aa0-8a02e5b78680",
   "metadata": {},
   "source": [
    "- We have 25000 rows and 2 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e0e7762-935d-40b3-9565-713a1ed9ad53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdc6cd-41cb-4c2c-9c6c-dfe9ac3ab749",
   "metadata": {},
   "source": [
    "- No null value present in both the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db24480-f900-4734-bde0-174db018f63b",
   "metadata": {},
   "source": [
    "### Lowercase All The Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddeba33d-186c-4f93-bf2b-ccc64a78b544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>i really seldom give either one or ten stars t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>this film is well cast, often silly and always...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>anna lives with her family in a new housing es...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>this was one of my favorite series when i was ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22038</th>\n",
       "      <td>for die-hard judy garland fans only. there are...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>herculis puaro is, in general, a well establis...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4618</th>\n",
       "      <td>ruggero deodato is often credited for inventin...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>possible spoilers, perhaps. i must say that \"c...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>at the start, this one is from england, so, of...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>what a joke. i am watching it on channel 1 and...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "2669   i really seldom give either one or ten stars t...  negative\n",
       "103    this film is well cast, often silly and always...  positive\n",
       "8367   anna lives with her family in a new housing es...  negative\n",
       "1119   this was one of my favorite series when i was ...  positive\n",
       "22038  for die-hard judy garland fans only. there are...  negative\n",
       "444    herculis puaro is, in general, a well establis...  negative\n",
       "4618   ruggero deodato is often credited for inventin...  negative\n",
       "487    possible spoilers, perhaps. i must say that \"c...  negative\n",
       "4842   at the start, this one is from england, so, of...  positive\n",
       "5617   what a joke. i am watching it on channel 1 and...  negative"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['review'] = imdb['review'].str.lower()\n",
    "imdb.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78f4452-c5f1-473c-a861-50c3656483d8",
   "metadata": {},
   "source": [
    "- Converting the all uppercase words and letters to lowercase, that can be observed in above output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865d86d-3031-438e-8145-596799359b70",
   "metadata": {},
   "source": [
    "# Preprocess Text Data(Remove punctuation, Perform Tokenization, Remove stopwords and Lemmatize/Stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a099d207-02b9-4f41-b8f7-6eeb168e866e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bhavinpatel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bhavinpatel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bhavinpatel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c79e66-62cc-4741-9e0d-0fc8f270b1ee",
   "metadata": {},
   "source": [
    "### Defining the Stop words and punctuation symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd996a45-3648-464e-97b8-464530864642",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'>', '.', '%', '@', '|', '#', '&', '$', \"'\", '/', '_', '^', '<', '``', ']', '(', '*', \"''\", '=', ':', '!', ')', '?', '{', '~', '[', '}', '\"', '-', ';', '\\\\', '+', ',', '`'} \n",
      "\n",
      "{'s', 'into', 'does', 'don', 'ours', 'what', 'have', 'at', 'by', 'until', \"mightn't\", 'our', 'to', 'hasn', 'for', 'he', \"she's\", 'him', 'themselves', 'now', 'had', 'further', 'hadn', 'his', 'when', \"you've\", 'most', 'isn', 'mustn', 'am', 'didn', 'very', 'above', 'with', 'she', 'will', 'wasn', 'up', 'through', 'other', 'weren', 'aren', 'itself', 'doing', 'haven', 'here', 'only', 'can', 'under', 'hers', 'myself', 'shan', 'you', \"haven't\", \"hasn't\", \"you'd\", 'wouldn', 'whom', \"that'll\", 'a', 'any', 'nor', 'these', 'out', 'before', 'it', 'did', 'during', \"should've\", 'ain', \"won't\", 're', 'yourselves', 'so', 'after', 'as', 'then', 'where', \"weren't\", 'your', 'more', \"shouldn't\", 'll', 'been', 'too', 'doesn', 'was', 'off', 'again', 'we', 'himself', 'being', 'in', \"doesn't\", 'shouldn', 'needn', \"wouldn't\", 'having', 'between', 'd', 'there', \"needn't\", \"you'll\", 'and', 'my', 'yours', 'or', 'an', 'me', 'their', 'her', 'over', 'yourself', \"mustn't\", \"wasn't\", 'once', 'its', 'each', 'few', 'about', 'm', 'such', 'has', 'if', 'why', 'but', 'y', 'should', 've', 'ourselves', 'theirs', 'because', 'them', 'those', 'on', \"isn't\", \"couldn't\", 'be', 'same', 'no', 'this', \"you're\", 'do', 'who', \"didn't\", 't', 'below', 'how', 'i', 'herself', \"aren't\", 'couldn', \"hadn't\", 'the', 'while', 'of', 'all', 'that', 'is', 'from', 'not', 'than', 'ma', 'are', 'own', 'down', 'against', 'just', 'o', 'they', 'won', 'were', \"shan't\", 'both', 'some', \"it's\", 'mightn', 'which', \"don't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#punctuation_symbols = set(string.punctuation)\n",
    "punctuation_symbols = {'.', ',', ';', ':', '?', '!', '-', '_', '(', ')', '[', ']', '{', '}', '/', '\\\\', '\"', \"'\", \"`\",\"''\", '``', ';', '+', '*', ':', ')', '=', '\"', '@', ',', '?', '|', '#', '^', ']', '!', '<', '&', '[', '-', \"'\", '/', '$', '.', '_', '`', '>', '\\\\', '~', '}', '{', '(', '%'}\n",
    "print(punctuation_symbols, '\\n')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd33c-360a-4f3e-9fdb-51ddb704aa24",
   "metadata": {},
   "source": [
    "- Above output displays the stopwords and punctuation symbols that we need to remove from the review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6c995f4-48bb-49a7-8975-56861ad04f6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_2 = imdb.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d216b-3b8b-4a0c-86be-92938ae8df79",
   "metadata": {},
   "source": [
    "- Creating the copy of dataframe to process all the operations."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7238c1b-134c-4f5e-8ceb-f72d67df7bf3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def count_punctuation(text):\n",
    "    return sum(text.count(p) for p in string.punctuation)\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "imdb_2['punctuation_count'] = imdb['review'].apply(count_punctuation)\n",
    "\n",
    "# Total punctuation count\n",
    "total_punctuation_count = imdb_2['punctuation_count'].sum()\n",
    "\n",
    "print(\"Total punctuation count:\", total_punctuation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de06b00-96b8-4a05-a6b8-a5f968b69c35",
   "metadata": {},
   "source": [
    "## Counting the punctuation and stopwords :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5599280d-d1ca-4f99-9ebc-075f76705a56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  i thought this was a wonderful way to spend ti...  positive\n",
       "1  probably my all-time favorite movie, a story o...  positive\n",
       "2  i sure would like to see a resurrection of a u...  positive\n",
       "3  this show was an amazing, fresh & innovative i...  negative\n",
       "4  encouraged by the positive comments about this...  negative"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02d2d80-3ac7-487c-b59e-80293f44aedb",
   "metadata": {},
   "source": [
    "### Def function for counting the punctuation and stopwords present in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc7ccca-2303-4567-a1b9-f9b3522490e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_punct(text):\n",
    "    punctuation_count = sum(1 for char in text if char in punctuation_symbols)\n",
    "    return punctuation_count\n",
    "\n",
    "def count_stopwords(text):\n",
    "    ret_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = ret_tokenizer.tokenize(text)\n",
    "    \n",
    "    stop_words_count = sum(1 for word in tokens if word.lower() in stopwords.words('english'))\n",
    "    return stop_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f89f04fa-40e4-4c76-b1c0-3b905ef35ee4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total punctuation count: 1313554\n"
     ]
    }
   ],
   "source": [
    "imdb_2['punctuation_count'] = imdb['review'].apply(count_punct)\n",
    "\n",
    "\n",
    "total_punctuation_count = imdb_2['punctuation_count'].sum()\n",
    "\n",
    "print(\"Total punctuation count:\", total_punctuation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58a90e0-1c85-4af3-a5be-98e690373771",
   "metadata": {},
   "source": [
    "- There are total 1313554 punctuation present in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41a88291-1ac1-4ed0-a3b8-5e3db7f9d362",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stopwords count: 2896187\n"
     ]
    }
   ],
   "source": [
    "imdb_2['Stopword_count'] = imdb['review'].apply(count_stopwords)\n",
    "\n",
    "total_stopwords_count = imdb_2['Stopword_count'].sum()\n",
    "\n",
    "print(\"Total Stopwords count:\", total_stopwords_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c92464f-fb20-43e0-a24d-0cb4413ff2bf",
   "metadata": {},
   "source": [
    "- There are total 2896187 stopwords present in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f8240-580b-49eb-9060-5b624ee1b053",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>Stopword_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  i thought this was a wonderful way to spend ti...  positive   \n",
       "1  probably my all-time favorite movie, a story o...  positive   \n",
       "2  i sure would like to see a resurrection of a u...  positive   \n",
       "3  this show was an amazing, fresh & innovative i...  negative   \n",
       "4  encouraged by the positive comments about this...  negative   \n",
       "\n",
       "   punctuation_count  Stopword_count  \n",
       "0                 40              83  \n",
       "1                 28              69  \n",
       "2                 12              86  \n",
       "3                 33              96  \n",
       "4                 31              64  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65e30dc-f242-4f01-bd59-abd1f65af6ab",
   "metadata": {},
   "source": [
    "- Added two columns punctuation_count and stopwords_count to analysis the count of punctuation and stopwords for each row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b4fe9d-bc8f-4653-947c-5b028ae4d77e",
   "metadata": {},
   "source": [
    "## Removing the punctuation :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbaf75-07e6-4823-bfa7-c3f80bd3ec91",
   "metadata": {},
   "source": [
    "### Def function for removing the punctuation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58cd8096-0394-46cd-9e23-fa0f89fb9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    for symbol in punctuation_symbols:\n",
    "        text = text.replace(symbol, '')\n",
    "    return text\n",
    "\n",
    "imdb_2['new_review'] = imdb_2['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51aa89b-7e94-437f-8ede-3889e75de753",
   "metadata": {},
   "source": [
    "#### Counting the punctuation after removing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed363ef-13ad-41fe-bc12-04ae74e14bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total punctuation count after removing the punctuation: 0\n"
     ]
    }
   ],
   "source": [
    "imdb_2['after_removing_punctuation_count'] = imdb_2['new_review'].apply(count_punct)\n",
    "\n",
    "total_after_removing_punctuation_count = imdb_2['after_removing_punctuation_count'].sum()\n",
    "\n",
    "print(\"Total punctuation count after removing the punctuation:\", total_after_removing_punctuation_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fc75a-4090-4ef0-a928-48ac12b030d7",
   "metadata": {},
   "source": [
    "- The output displays the count of punctuation after removing it means we have successfully removed the punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf37ff-ae97-4215-b4ad-32caf41d0ba1",
   "metadata": {},
   "source": [
    "## Example (Before and after removing the punctuations): "
   ]
  },
  {
   "cell_type": "raw",
   "id": "090df75d-8d55-4f5b-be2e-54c23a750512",
   "metadata": {
    "tags": []
   },
   "source": [
    "imdb['review'][3]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "057330be-f311-468e-aafe-120f7a7c8f37",
   "metadata": {
    "tags": []
   },
   "source": [
    "imdb_2['review'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47afd1b1-908b-461e-93a7-ed471d84d5db",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169b7f1f-6e39-4770-b2b2-bdb1af04213d",
   "metadata": {},
   "source": [
    "## Tokenization and Removing the stopwords :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5d4c6d-a077-442a-8017-5069657e1907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7527e8fb-5d16-4ae5-a321-76ec16ac9b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imdb_2['new_review'] = imdb_2['new_review'].apply(tokenize_and_remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "503f12a3-1ecb-4e84-b6be-3d05fc06ddb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def after_count_stopwords(text):\n",
    "\n",
    "    tokens = text\n",
    "    \n",
    "    stop_words_count = sum(1 for word in tokens if word.lower() in stopwords.words('english'))\n",
    "    return stop_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2116b9f-0b69-4ec3-9c58-5e03b872d029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Stopwords count after tokenizing and removing the stopwords: 0\n"
     ]
    }
   ],
   "source": [
    "imdb_2['after_Stopword_count'] = imdb_2['new_review'].apply(after_count_stopwords)\n",
    "\n",
    "after_stopwords_count = imdb_2['after_Stopword_count'].sum()\n",
    "\n",
    "print(\"Total Stopwords count after tokenizing and removing the stopwords:\", after_stopwords_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2636388-1071-4b9a-9df9-b3491a3f80b7",
   "metadata": {},
   "source": [
    "- The output displays the count of stopwords after removing it means we have successfully removed all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29851a3a-cfe5-43c8-b4c0-c3494f2df31c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>Stopword_count</th>\n",
       "      <th>new_review</th>\n",
       "      <th>after_removing_punctuation_count</th>\n",
       "      <th>after_Stopword_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>64</td>\n",
       "      <td>[encouraged, positive, comments, film, looking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>phil the alien is one of those quirky films wh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>[phil, alien, one, quirky, films, humour, base...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i saw this movie when i was about 12 when it c...</td>\n",
       "      <td>negative</td>\n",
       "      <td>32</td>\n",
       "      <td>96</td>\n",
       "      <td>[saw, movie, 12, came, recall, scariest, scene...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>so im not a big fan of boll's work but then ag...</td>\n",
       "      <td>negative</td>\n",
       "      <td>86</td>\n",
       "      <td>178</td>\n",
       "      <td>[im, big, fan, bolls, work, many, enjoyed, mov...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>this a fantastic movie of three prisoners who ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>[fantastic, movie, three, prisoners, become, f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>this movie made it into one of my top 10 most ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>117</td>\n",
       "      <td>112</td>\n",
       "      <td>[movie, made, one, top, 10, awful, movies, hor...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  i thought this was a wonderful way to spend ti...  positive   \n",
       "1  probably my all-time favorite movie, a story o...  positive   \n",
       "2  i sure would like to see a resurrection of a u...  positive   \n",
       "3  this show was an amazing, fresh & innovative i...  negative   \n",
       "4  encouraged by the positive comments about this...  negative   \n",
       "5  phil the alien is one of those quirky films wh...  negative   \n",
       "6  i saw this movie when i was about 12 when it c...  negative   \n",
       "7  so im not a big fan of boll's work but then ag...  negative   \n",
       "8  this a fantastic movie of three prisoners who ...  positive   \n",
       "9  this movie made it into one of my top 10 most ...  negative   \n",
       "\n",
       "   punctuation_count  Stopword_count  \\\n",
       "0                 40              83   \n",
       "1                 28              69   \n",
       "2                 12              86   \n",
       "3                 33              96   \n",
       "4                 31              64   \n",
       "5                 35              43   \n",
       "6                 32              96   \n",
       "7                 86             178   \n",
       "8                  7              25   \n",
       "9                117             112   \n",
       "\n",
       "                                          new_review  \\\n",
       "0  [thought, wonderful, way, spend, time, hot, su...   \n",
       "1  [probably, alltime, favorite, movie, story, se...   \n",
       "2  [sure, would, like, see, resurrection, dated, ...   \n",
       "3  [show, amazing, fresh, innovative, idea, 70s, ...   \n",
       "4  [encouraged, positive, comments, film, looking...   \n",
       "5  [phil, alien, one, quirky, films, humour, base...   \n",
       "6  [saw, movie, 12, came, recall, scariest, scene...   \n",
       "7  [im, big, fan, bolls, work, many, enjoyed, mov...   \n",
       "8  [fantastic, movie, three, prisoners, become, f...   \n",
       "9  [movie, made, one, top, 10, awful, movies, hor...   \n",
       "\n",
       "   after_removing_punctuation_count  after_Stopword_count  \n",
       "0                                 0                     0  \n",
       "1                                 0                     0  \n",
       "2                                 0                     0  \n",
       "3                                 0                     0  \n",
       "4                                 0                     0  \n",
       "5                                 0                     0  \n",
       "6                                 0                     0  \n",
       "7                                 0                     0  \n",
       "8                                 0                     0  \n",
       "9                                 0                     0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18412d30-8fea-4d6c-a03d-57520b26e851",
   "metadata": {},
   "source": [
    "- Above is the Dataframe after removing the stopword and punctuation, for analysing the count of both before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ac1c654-a708-4934-8826-1db38bd8dcf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this show was an amazing, fresh & innovative idea in the 70's when it first aired. the first 7 or 8 years were brilliant, but things dropped off after that. by 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />it's truly disgraceful how far this show has fallen. the writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. i find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. how can one recognize such brilliance and then see fit to replace it with such mediocrity? i felt i must give 2 stars out of respect for the original cast that made this show such a huge success. as it is now, the show is just awful. i can't believe it's still on the air.\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccea401e-00f0-4f59-8b9e-869843f8c89c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'amazing', 'fresh', 'innovative', 'idea', '70s', 'first', 'aired', 'first', '7', '8', 'years', 'brilliant', 'things', 'dropped', '1990', 'show', 'really', 'funny', 'anymore', 'continued', 'decline', 'complete', 'waste', 'time', 'todaybr', 'br', 'truly', 'disgraceful', 'far', 'show', 'fallen', 'writing', 'painfully', 'bad', 'performances', 'almost', 'bad', 'mildly', 'entertaining', 'respite', 'guesthosts', 'show', 'probably', 'wouldnt', 'still', 'air', 'find', 'hard', 'believe', 'creator', 'handselected', 'original', 'cast', 'also', 'chose', 'band', 'hacks', 'followed', 'one', 'recognize', 'brilliance', 'see', 'fit', 'replace', 'mediocrity', 'felt', 'must', 'give', '2', 'stars', 'respect', 'original', 'cast', 'made', 'show', 'huge', 'success', 'show', 'awful', 'cant', 'believe', 'still', 'air']\n"
     ]
    }
   ],
   "source": [
    "text_token = imdb_2['new_review'][3]\n",
    "print(text_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192d583-2d0a-4d7b-accf-fa9df7f7c42e",
   "metadata": {},
   "source": [
    "- Above two are the example of a one review after Preprocess Text Data(Remove punctuation, Perform Tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce4bc6c-325f-43f6-9c16-a6bf29a4aa99",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4484ec09-a19f-45f9-b145-6ce620c6f18c",
   "metadata": {},
   "source": [
    "## Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ebd1e04-43d6-4856-9bf3-7112e4cb83a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = text\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "imdb_2['stemmed_text'] = imdb_2['new_review'].apply(tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67bfabb-3a2a-4feb-b2ed-fa3262d117b9",
   "metadata": {},
   "source": [
    "## lim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e067399-a7b0-49a4-bc9e-5b511487eb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_and_lemmatize(text):\n",
    "\n",
    "    tokens = text\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "imdb_2['lemmatized_text'] = imdb_2['new_review'].apply(tokenize_and_lemmatize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e00e9-a36a-48ec-8b0a-37a044276548",
   "metadata": {},
   "source": [
    "#### Output after tokenizing and removing the punctuation and stopwords :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "497551aa-80ab-48c2-91cd-c3e387976411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'amazing', 'fresh', 'innovative', 'idea', '70s', 'first', 'aired', 'first', '7', '8', 'years', 'brilliant', 'things', 'dropped', '1990', 'show', 'really', 'funny', 'anymore', 'continued', 'decline', 'complete', 'waste', 'time', 'todaybr', 'br', 'truly', 'disgraceful', 'far', 'show', 'fallen', 'writing', 'painfully', 'bad', 'performances', 'almost', 'bad', 'mildly', 'entertaining', 'respite', 'guesthosts', 'show', 'probably', 'wouldnt', 'still', 'air', 'find', 'hard', 'believe', 'creator', 'handselected', 'original', 'cast', 'also', 'chose', 'band', 'hacks', 'followed', 'one', 'recognize', 'brilliance', 'see', 'fit', 'replace', 'mediocrity', 'felt', 'must', 'give', '2', 'stars', 'respect', 'original', 'cast', 'made', 'show', 'huge', 'success', 'show', 'awful', 'cant', 'believe', 'still', 'air']\n"
     ]
    }
   ],
   "source": [
    "text_token = imdb_2['new_review'][3]\n",
    "print(text_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480cb9d7-0f2c-4131-ba60-2a672284ed4c",
   "metadata": {},
   "source": [
    "#### Output after tokenizing, removing the punctuation and stopwords, & applying the Lemmatization :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "452a02f6-be53-4abb-a171-4ba464c38790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'amazing', 'fresh', 'innovative', 'idea', '70', 'first', 'aired', 'first', '7', '8', 'year', 'brilliant', 'thing', 'dropped', '1990', 'show', 'really', 'funny', 'anymore', 'continued', 'decline', 'complete', 'waste', 'time', 'todaybr', 'br', 'truly', 'disgraceful', 'far', 'show', 'fallen', 'writing', 'painfully', 'bad', 'performance', 'almost', 'bad', 'mildly', 'entertaining', 'respite', 'guesthosts', 'show', 'probably', 'wouldnt', 'still', 'air', 'find', 'hard', 'believe', 'creator', 'handselected', 'original', 'cast', 'also', 'chose', 'band', 'hack', 'followed', 'one', 'recognize', 'brilliance', 'see', 'fit', 'replace', 'mediocrity', 'felt', 'must', 'give', '2', 'star', 'respect', 'original', 'cast', 'made', 'show', 'huge', 'success', 'show', 'awful', 'cant', 'believe', 'still', 'air']\n"
     ]
    }
   ],
   "source": [
    "l_text = imdb_2['lemmatized_text'][3]\n",
    "print(l_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d625e3-a316-45ff-a9a4-202048b83a0d",
   "metadata": {},
   "source": [
    "#### Output after tokenizing, removing the punctuation and stopwords, & applying the Stemming :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71a2dec8-cc2c-40b4-a337-ad7005a52a93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['show', 'amaz', 'fresh', 'innov', 'idea', '70', 'first', 'air', 'first', '7', '8', 'year', 'brilliant', 'thing', 'drop', '1990', 'show', 'realli', 'funni', 'anymor', 'continu', 'declin', 'complet', 'wast', 'time', 'todaybr', 'br', 'truli', 'disgrac', 'far', 'show', 'fallen', 'write', 'pain', 'bad', 'perform', 'almost', 'bad', 'mildli', 'entertain', 'respit', 'guesthost', 'show', 'probabl', 'wouldnt', 'still', 'air', 'find', 'hard', 'believ', 'creator', 'handselect', 'origin', 'cast', 'also', 'chose', 'band', 'hack', 'follow', 'one', 'recogn', 'brillianc', 'see', 'fit', 'replac', 'mediocr', 'felt', 'must', 'give', '2', 'star', 'respect', 'origin', 'cast', 'made', 'show', 'huge', 'success', 'show', 'aw', 'cant', 'believ', 'still', 'air']\n"
     ]
    }
   ],
   "source": [
    "s_text = imdb_2['stemmed_text'][3]\n",
    "print(s_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d55e9cd-52d7-4b14-82ca-c12af34e959f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>Stopword_count</th>\n",
       "      <th>new_review</th>\n",
       "      <th>after_removing_punctuation_count</th>\n",
       "      <th>after_Stopword_count</th>\n",
       "      <th>stemmed_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>40</td>\n",
       "      <td>83</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[thought, wonder, way, spend, time, hot, summe...</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[probabl, alltim, favorit, movi, stori, selfle...</td>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "      <td>12</td>\n",
       "      <td>86</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sure, would, like, see, resurrect, date, seah...</td>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "      <td>33</td>\n",
       "      <td>96</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70s, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[show, amaz, fresh, innov, idea, 70, first, ai...</td>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "      <td>31</td>\n",
       "      <td>64</td>\n",
       "      <td>[encouraged, positive, comments, film, looking...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[encourag, posit, comment, film, look, forward...</td>\n",
       "      <td>[encouraged, positive, comment, film, looking,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  i thought this was a wonderful way to spend ti...  positive   \n",
       "1  probably my all-time favorite movie, a story o...  positive   \n",
       "2  i sure would like to see a resurrection of a u...  positive   \n",
       "3  this show was an amazing, fresh & innovative i...  negative   \n",
       "4  encouraged by the positive comments about this...  negative   \n",
       "\n",
       "   punctuation_count  Stopword_count  \\\n",
       "0                 40              83   \n",
       "1                 28              69   \n",
       "2                 12              86   \n",
       "3                 33              96   \n",
       "4                 31              64   \n",
       "\n",
       "                                          new_review  \\\n",
       "0  [thought, wonderful, way, spend, time, hot, su...   \n",
       "1  [probably, alltime, favorite, movie, story, se...   \n",
       "2  [sure, would, like, see, resurrection, dated, ...   \n",
       "3  [show, amazing, fresh, innovative, idea, 70s, ...   \n",
       "4  [encouraged, positive, comments, film, looking...   \n",
       "\n",
       "   after_removing_punctuation_count  after_Stopword_count  \\\n",
       "0                                 0                     0   \n",
       "1                                 0                     0   \n",
       "2                                 0                     0   \n",
       "3                                 0                     0   \n",
       "4                                 0                     0   \n",
       "\n",
       "                                        stemmed_text  \\\n",
       "0  [thought, wonder, way, spend, time, hot, summe...   \n",
       "1  [probabl, alltim, favorit, movi, stori, selfle...   \n",
       "2  [sure, would, like, see, resurrect, date, seah...   \n",
       "3  [show, amaz, fresh, innov, idea, 70, first, ai...   \n",
       "4  [encourag, posit, comment, film, look, forward...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [thought, wonderful, way, spend, time, hot, su...  \n",
       "1  [probably, alltime, favorite, movie, story, se...  \n",
       "2  [sure, would, like, see, resurrection, dated, ...  \n",
       "3  [show, amazing, fresh, innovative, idea, 70, f...  \n",
       "4  [encouraged, positive, comment, film, looking,...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedb7996-42d3-45c7-a3ef-f02353c74b3f",
   "metadata": {},
   "source": [
    "- In the context of movie review classification, where the primary goal is often to identify sentiment or opinions expressed in the reviews, lemmatization might be preferred over stemming due to its ability to produce more meaningful tokens that better capture the semantics of the words. \n",
    "\n",
    "- However, it's essential to experiment with both approaches and evaluate their performance using appropriate metrics to determine which one works best for a specific dataset and classification task. Additionally, the choice of lemmatization or stemming can also depend on the specific requirements of the downstream classification algorithm and the computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de2c95-7fc6-42f4-a9d6-6ab2fe982789",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2e3b2-64da-448b-bdda-b34da5ecaa73",
   "metadata": {},
   "source": [
    "# Perform TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f2dc0b8-1e46-44e8-8bd3-bfc54862ffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: (25000, 113304)(number_of_samples, vocabulary_size)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "clean_data = [' '.join(doc) for doc in imdb_2['lemmatized_text']]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(clean_data)\n",
    "\n",
    "\n",
    "print(f'Output: {(X_tfidf.shape)}(number_of_samples, vocabulary_size)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9152b-1700-4b6e-bb6a-4adcc3413f80",
   "metadata": {},
   "source": [
    "- Converting the Tokenization and lemmatized data to the numeric value to perform the next operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539c91fb-a389-427b-902f-6177071ad8c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4c650-4bda-435f-a586-814ac458c060",
   "metadata": {},
   "source": [
    "# Exploring parameter settings using GridSearchCV on Random Forest & Gradient Boosting Classifier. Use Xgboost instead of Gradient Boosting if it's taking a very long time in GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ba0f1-6a57-4550-b308-227341e13e7c",
   "metadata": {},
   "source": [
    "## Evaluation before reducing the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fecf39a-0414-4ef8-98b2-8b328510e3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "20d8a4ec-d413-4a6a-b3e4-404cb078cff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[encouraged, positive, comment, film, looking,...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text    target\n",
       "0  [thought, wonderful, way, spend, time, hot, su...  positive\n",
       "1  [probably, alltime, favorite, movie, story, se...  positive\n",
       "2  [sure, would, like, see, resurrection, dated, ...  positive\n",
       "3  [show, amazing, fresh, innovative, idea, 70, f...  negative\n",
       "4  [encouraged, positive, comment, film, looking,...  negative"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = {'cleaned_text': imdb_2['lemmatized_text'], 'target': imdb_2['sentiment']} \n",
    "main_df = pd.DataFrame(cleaned_data)\n",
    "main_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4f294230-4328-4106-b640-b6742f55772b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "negative    12500\n",
      "positive    12500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(main_df.groupby('target').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b53e835-8d4c-4dff-8275-6270bd1e2a47",
   "metadata": {},
   "source": [
    "- Checking the count of the each target column values, model will not perform bias as both the values are same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8f0c7-9975-443e-bd86-06dac3b4380d",
   "metadata": {},
   "source": [
    "## Mapping the Tareget Column :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2afeb7c0-a0d0-473c-86cf-34445c7aa98a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[probably, alltime, favorite, movie, story, se...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[sure, would, like, see, resurrection, dated, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[show, amazing, fresh, innovative, idea, 70, f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[encouraged, positive, comment, film, looking,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>[first, tuned, morning, news, thought, wow, fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>[got, one, week, ago, love, modern, light, fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>[bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>[catholic, taught, parochial, elementary, scho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>[one, expects, star, trek, movie, high, art, f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            cleaned_text  target\n",
       "0      [thought, wonderful, way, spend, time, hot, su...       1\n",
       "1      [probably, alltime, favorite, movie, story, se...       1\n",
       "2      [sure, would, like, see, resurrection, dated, ...       1\n",
       "3      [show, amazing, fresh, innovative, idea, 70, f...       0\n",
       "4      [encouraged, positive, comment, film, looking,...       0\n",
       "...                                                  ...     ...\n",
       "24995  [first, tuned, morning, news, thought, wow, fi...       0\n",
       "24996  [got, one, week, ago, love, modern, light, fil...       1\n",
       "24997  [bad, plot, bad, dialogue, bad, acting, idioti...       0\n",
       "24998  [catholic, taught, parochial, elementary, scho...       0\n",
       "24999  [one, expects, star, trek, movie, high, art, f...       0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "name_mapping = { 'negative':0,'positive':1 }\n",
    "main_df[\"target\"] = main_df['target'].map(name_mapping)\n",
    "display(main_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d71dbb1-e67a-47cf-b8a8-c31142288e19",
   "metadata": {},
   "source": [
    "- Mapping the target column converting the str to int value to enabling better decision-making, analysis, and operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09371f30-e9b1-48e4-a40c-29f3e40f487e",
   "metadata": {},
   "source": [
    "### Splitting data into (60, 40) ratio 60 for training and rest for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1ad03a65-72ba-49c9-a626-450a9e1de13c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Test split ratio is : [60, 40] \n",
      "\n",
      "Training set:\n",
      "X_train shape: (15000,)\n",
      "y_train shape: (15000,)\n",
      "\n",
      "Testing set:\n",
      "X_test shape: (10000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(main_df['cleaned_text'], main_df['target'], test_size=0.4, stratify=main_df['target'], random_state=78)\n",
    "\n",
    "print('Train Test split ratio is : [60, 40]','\\n')\n",
    "print(\"Training set:\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", Y_train.shape)\n",
    "\n",
    "\n",
    "print(\"\\nTesting set:\")\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5007d21e-e523-40e9-bf93-94cdf657ece3",
   "metadata": {},
   "source": [
    "- We have 15000 rows for training the model and 10000 rows for the testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211caa4-7c49-43cf-bb40-4cee82609c3a",
   "metadata": {},
   "source": [
    "### Applying the TFIDF Vectorization method to the X_test and X_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fdd599aa-c419-474f-8b6e-885f5bde1730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = [' '.join(doc) for doc in X_train]\n",
    "X_test = [' '.join(doc) for doc in X_test]\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb26c2-57cf-476c-a81a-57bd84751d14",
   "metadata": {},
   "source": [
    "### Defining the classifier models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "012ed148-682f-41c5-9889-4d41c7904566",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "xgb_classifier = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd40592-faf3-4112-ab85-94dc3796acaa",
   "metadata": {},
   "source": [
    "### Defining parameters for the the model to perform Grid Search CV :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a0812c68-649e-49a1-bd48-c7a7e24c371e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_xgb = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583f8c8-ae18-4743-98cb-b3d431a44c8e",
   "metadata": {},
   "source": [
    "## Perform GridSearchCV for Random Forest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "76f227f8-d03f-4268-99bf-42e18111130b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency for Random Forest:  1600.5208892822266\n"
     ]
    }
   ],
   "source": [
    "grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, n_jobs=5)\n",
    "start_time1 = time.time()\n",
    "grid_search_rf.fit(X_train_tfidf, Y_train)\n",
    "latency = time.time() - start_time1\n",
    "print(\"Latency for Random Forest: \", latency )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b46ef-7c6c-43cf-b7a2-2fdd8c9b3a47",
   "metadata": {},
   "source": [
    "#### Converting the latency time to minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "be0c4845-e967-4014-8c06-3bd2454c0e35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest took 26.675348154703777 minutes to perform the Grid search CV\n"
     ]
    }
   ],
   "source": [
    "tak_time1 = latency/60\n",
    "print(f\"Random Forest took {tak_time1} minutes to perform the Grid search CV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6526d200-c49e-4030-b0d8-936a07e11013",
   "metadata": {},
   "source": [
    "#### Best Parameters and score for Random Forest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e91d454f-d21a-4c7d-9e44-3590215db47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters obtained after performing the GSVC : {'max_depth': None, 'min_samples_leaf': 4, 'min_samples_split': 2, 'n_estimators': 300} \n",
      "\n",
      "Model Best score : 85.55\n"
     ]
    }
   ],
   "source": [
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "print(\"Best Parameters obtained after performing the GSVC :\", best_params_rf, '\\n')\n",
    "print(\"Model Best score :\", round(best_score_rf, 4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba8dcf1-7411-468f-b5dd-e7bf8bfb89e7",
   "metadata": {},
   "source": [
    "- Perform the Grid search cv for Random forest classifier which almost took 26 minutes to obtain the best parameters.\n",
    "- Model best score is 85.55 %, which is not really bad in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c47080a-98ba-4f2b-a99b-0453aace4350",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3277e03c-6d4c-4448-b1c4-f0b8304e5cf1",
   "metadata": {},
   "source": [
    "## Perform GridSearchCV for Gradient Boosting"
   ]
  },
  {
   "cell_type": "raw",
   "id": "546a1e18-169d-480a-bdf2-dc33a70a0ce0",
   "metadata": {
    "tags": []
   },
   "source": [
    "grid_search_gb = GridSearchCV(gb_classifier, param_grid_gb, cv=5, n_jobs=-5)\n",
    "start_time2 = time.time()   \n",
    "grid_search_gb.fit(X_train_tfidf, Y_train)\n",
    "print(grid_search_gb.refit_time_, '\\n')\n",
    "latency2 = time.time() - start_time2 "
   ]
  },
  {
   "cell_type": "raw",
   "id": "61885b75-8d8d-4149-8644-742fde11e5a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "print(\"Latency for Gradient Boosting: \", latency2)\n",
    "best_params = grid_search_xgb.best_params_\n",
    "best_score = grid_search_xgb.best_score_\n",
    "\n",
    "print(\"Best performing model parameters:\")\n",
    "print(best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40292ccd-a086-4203-989f-da47d7fd745a",
   "metadata": {},
   "source": [
    "### Going to use the XGB, reason because the Gradient boosting was taking so long to perform GSCV I have almost wait for around 8-10 hours for the result but i did not received the output. So later i decide to move with XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d7426-ae11-4573-9ba8-c308605a83c6",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38af4c96-04ef-4088-a753-e2580409db72",
   "metadata": {},
   "source": [
    "## Perform GridSearchCV for XGB :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9d7d6ab6-1039-4619-9300-4fd189adca3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency for Random Forest:  7599.761258840561\n"
     ]
    }
   ],
   "source": [
    "grid_search_xgb = GridSearchCV(xgb_classifier, param_grid_xgb, cv=5, n_jobs=-5)\n",
    "\n",
    "start_time2 = time.time()\n",
    "\n",
    "grid_search_xgb.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "latency2 = time.time() - start_time2\n",
    "print(\"Latency for Random Forest: \", latency2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b71958-2126-4244-82d5-23816e91fd19",
   "metadata": {},
   "source": [
    "#### Converting the latency time to minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6efe7984-72fe-4d0d-ba6e-ced287d2c726",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB took 126.66268764734268 minutes to perform the Grid search CV\n"
     ]
    }
   ],
   "source": [
    "tak_time = latency2/60\n",
    "print(f\"XGB took {tak_time} minutes to perform the Grid search CV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f7453-7cdd-4e11-a38f-a2af19a2c7f8",
   "metadata": {},
   "source": [
    "#### Time required to perform GSCV using XGB classifier is 2 hours and around 6 minutes to complete the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2c031d34-1a20-4aa7-be4a-a539876cc9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters obtained after performing the GSVC : {'gamma': 0.2, 'learning_rate': 0.2, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 300} \n",
      "\n",
      "Model Best score : 85.89\n"
     ]
    }
   ],
   "source": [
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_score_xgb = grid_search_xgb.best_score_\n",
    "\n",
    "print(\"Best Parameters obtained after performing the GSVC :\", best_params_xgb, '\\n')\n",
    "print(\"Model Best score :\", round(best_score_xgb, 4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bc2b27-221b-4fda-a0e5-6c0805d5203e",
   "metadata": {},
   "source": [
    "- Perform the Grid search cv for Random forest classifier which almost took 2 hours to complete the process.\n",
    "- Model best score is 85.89 %, which is not really bad in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66d657-8ecc-4e2f-bfe0-90abb13cda35",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdd7c1-3c03-457d-a0dd-f38363205ab3",
   "metadata": {},
   "source": [
    "# Perform Final evaluation of models on the best parameter settings using the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aaa0c282-adb9-4819-9005-7823729975b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest model: 86.0\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86      5000\n",
      "           1       0.86      0.86      0.86      5000\n",
      "\n",
      "    accuracy                           0.86     10000\n",
      "   macro avg       0.86      0.86      0.86     10000\n",
      "weighted avg       0.86      0.86      0.86     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf_model = RandomForestClassifier(**best_params_rf)\n",
    "\n",
    "best_rf_model.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "y_pred_rf = best_rf_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_rf = accuracy_score(Y_test, y_pred_rf)\n",
    "print(\"Accuracy of Random Forest model:\", round(accuracy_rf, 2)*100)\n",
    "\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(Y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "925bada1-25d4-42ad-8b3b-0f4d4bc1af19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of XGB model: 86.59\n",
      " XGB Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.85      0.86      5000\n",
      "           1       0.85      0.88      0.87      5000\n",
      "\n",
      "    accuracy                           0.87     10000\n",
      "   macro avg       0.87      0.87      0.87     10000\n",
      "weighted avg       0.87      0.87      0.87     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_XGB_model = xgb.XGBClassifier(**best_params_xgb)\n",
    "\n",
    "best_XGB_model.fit(X_train_tfidf, Y_train)\n",
    "\n",
    "y_pred_xgb = best_XGB_model.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_xgb = accuracy_score(Y_test, y_pred_xgb)\n",
    "print(\"Accuracy of XGB model:\", round(accuracy_xgb, 4)*100)\n",
    "\n",
    "print(\" XGB Classification Report:\")\n",
    "print(classification_report(Y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99644f85-5199-43ec-a5b4-29f38df88115",
   "metadata": {},
   "source": [
    "### Comparing both the model performance :\n",
    "- While performing the Grid search cv Random forest model took less time than the XGB. With the time differance of 1 hour and around 36 minutes.\n",
    "#### Evaluation based on testing data set for both the models :\n",
    "\n",
    "- Both model perform best also the accuracy is almost same for both.\n",
    "- Accuracy difference is around 0.59 percent.\n",
    "- But considering the classification report results XGB perform quite better than Randon forest.\n",
    "\n",
    "\n",
    "##### In my opinion i will go with the Random Forest model as the model did not take too long to perform Grid search cv compare to XGB as already mention above that the model took almost 2 hours and 36 minutes. Also I don't see major differance in accuracy. \n",
    "    - Also i want to mention one thing that while performing the same task with the Gradient Boosting i wait for around 8-10 hours for receiving the output so i decided to go with XGB. But that also take long time perform the same operation.\n",
    "    - So considering the time factor the Random Forest is best than XGB and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b500ae-1e2b-4f97-a342-4390761eb00d",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224f999-c3c0-49f6-9e61-8856e649d381",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
